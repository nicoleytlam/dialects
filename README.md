# Evaluating LLMsâ€™ Grammatical Comprehension  Across Dialects
## Fall 2024: LING 380/780: Topics in Computational Linguistics: Neural Network Models of Linguistic Structure
### By Nicole Lam and Katelyn DeKeersgieter

This repository provides a Seq2Seq model tools to analyze LLM's grammatical comprehension of different English dialects. It includes data preprocessing, model training, evaluation, and simulation scripts.

## Directory Structure

- **`dialects/`**: Main directory containing the code and resources.
  - **`metrics.py`**: Scripts for calculating model performance metrics.
  - **`analysis.py`**: Tools for in-depth analysis of model outputs.
  - **`encoder_decoder.py`**: Implementation of the seq2seq architecture.
  - **`data_loader.py`**: Handles data loading and preprocessing.
  - **`helper.py`**: Utility functions to support various tasks.
  - **`train.py`**: Main script for training models.
  - **`simulation.ipynb`**: **Central file for running the model and generating results.** (See below for more details.)
  - **`_utils.py`**: Additional utility functions for internal usage.

- **`data/`**: Dataset files.
  - `*.csv`: CSV files for training, validation, and testing.
  - `*.train`, `*.dev`, `*.test`: Data splits for different dialects.

- **`.git/`**: Git metadata.
- **Output Files**:
  - `aave_A.pt`, `aave_B.pt`, `aave_C.pt`: Models trained on African American Vernacular English (AAVE) data.
  - `standard_A.pt`, `standard_B.pt`, `standard_C.pt`: Models trained on Standard American English data.
  - `mix_A.pt`, `mix_B.pt`, `mix_C.pt`: Models trained on mixed dialect (AAVE + SAE) data.

## Getting Started

### Prerequisites

- Python 3.8 or higher
- PyTorch
- Jupyter Notebook (for running simulations)
- Other dependencies can be installed via:

```bash
pip install -r requirements.txt
```

### Installation

Clone the repository:

```bash
git clone <repository-url>
cd dialects
```

### Usage

For ease of use, all model results and graphical visualizations can be generated by executing `simulation.ipynb` to execute the model and generate results.

---

## Simulation Workflow (`simulation.ipynb`)

The `simulation.ipynb` file serves as the **primary interface for running models** and analyzing their outputs. It provides an end-to-end workflow for loading pre-trained models, running predictions, and visualizing results. Here's a breakdown of its functionality:

1. **Environment Setup**:
   - Initializes the required libraries and imports utility functions.

2. **Dataset Integration**:
   - Loads dialect-specific datasets (e.g., `aave.train`, `standard.test`, `mix.dev`) using `data_loader.py` and separates training, dev, and test data..
   - Preprocesses and tokenizes the data for the model.

3. **Model Execution**:
   - Runs the Seq2seq model with 3 sets of different parameters (models A, B, C).
   - Saves the model trained on various datasets in specific `*.pt` files.

4. **Simulation Scenarios**:
   - Allows the user to test specific dialect test-scenarios (e.g., comparing AAVE and standard English outputs) and evaluate their accuracy based on three different models

5. **Visualization**:
   - Provides plots and tables to analyze metrics such as training and test accuracy across epochs and trials
   - Compares performance across dialect-specific models and datasets.

6. **Output Generation**:
   - Saves visualizations directly within the notebook.

### Running the Simulation

To run the simulation:

1. Open the `simulation.ipynb` file in Jupyter Notebook:

   ```bash
   jupyter notebook simulation.ipynb
   ```

2. Follow the step-by-step instructions in the notebook cells:
   - Load a pre-trained model by specifying its path.
   - Select and load the dataset for evaluation.
   - Run the simulation and review the outputs.


---
## Contributing

Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes.

## License

This project is licensed under the MIT License. See `LICENSE` for details.

## Contact

For questions or suggestions, please reach out to nicole.lam@yale.edu or katelyn.dekeersgieter@yale.edu.
